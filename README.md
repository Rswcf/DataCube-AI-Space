<div align="center">

# ğŸ§Š Data Cube AI

### Your daily AI news, curated by AI.

**Bilingual (DE/EN) AI news aggregator** that curates tech breakthroughs, investment deals, practical tips, and YouTube videos â€” powered by a 4-stage LLM pipeline.

[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Live Demo](https://img.shields.io/badge/demo-datacubeai.space-brightgreen)](https://www.datacubeai.space)
[![CI](https://img.shields.io/github/actions/workflow/status/Rswcf/DataCube-AI-Space/ci.yml?label=CI)](https://github.com/Rswcf/DataCube-AI-Space/actions)

[English](README.md) | [ç®€ä½“ä¸­æ–‡](docs/README.zh-CN.md) | [Deutsch](docs/README.de.md) | [FranÃ§ais](docs/README.fr.md) | [EspaÃ±ol](docs/README.es.md) | [PortuguÃªs](docs/README.pt-BR.md) | [æ—¥æœ¬èª](docs/README.ja.md) | [í•œêµ­ì–´](docs/README.ko.md)

</div>

---

## What is Data Cube AI?

Data Cube AI automatically collects, classifies, and summarizes AI news from **22 RSS feeds**, **Hacker News**, and **YouTube** â€” then presents it in a clean bilingual (German/English) interface with daily and weekly views.

**Live at [datacubeai.space](https://www.datacubeai.space)** â€” no login required.

## Features

- **Tech Feed** â€” AI/ML breakthroughs with embedded YouTube videos and impact ratings
- **Investment Tracker** â€” Primary funding rounds, secondary market data (live stock prices via Polygon.io), and M&A deals
- **Practical Tips** â€” Curated from 14 Reddit communities and expert blogs
- **Bilingual** â€” Every article in both German and English
- **Daily + Weekly** â€” Automated daily collection with weekly rollup views
- **AI Chat** â€” Ask questions about the current week's AI news
- **AI Reports** â€” One-click streaming report with export to Word, HTML, Markdown, Text, or JSON
- **SEO/GEO Optimized** â€” SSR pages, JSON-LD structured data, Atom feed, llms.txt, sitemap
- **Accessible** â€” WCAG-compliant: 44px touch targets, focus-visible, ARIA, prefers-reduced-motion, skip links
- **Mobile-First** â€” Dynamic viewport, safe area insets, touch-optimized navigation, body scroll lock on overlays

## Architecture

```
Frontend (Vercel)                    Backend (Railway)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Next.js 16         â”‚    REST     â”‚  FastAPI + PostgreSQL        â”‚
â”‚  React 19           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                              â”‚
â”‚  Tailwind CSS 4     â”‚    API      â”‚  4-Stage Pipeline:           â”‚
â”‚  Shadcn/ui          â”‚             â”‚  1. Fetch (RSS, HN, YouTube) â”‚
â”‚                     â”‚             â”‚  2. Classify (LLM)           â”‚
â”‚  Pages:             â”‚             â”‚  3. Process (LLM, parallel)  â”‚
â”‚  â€¢ Tech Feed        â”‚             â”‚  4. Save to PostgreSQL       â”‚
â”‚  â€¢ Investment Feed  â”‚             â”‚                              â”‚
â”‚  â€¢ Tips Feed        â”‚             â”‚  Data Sources:               â”‚
â”‚  â€¢ AI Chat          â”‚             â”‚  â€¢ 22 RSS Feeds              â”‚
â”‚  â€¢ AI Reports       â”‚             â”‚  â€¢ Hacker News (Algolia)     â”‚
â”‚  â€¢ SSR Week Pages   â”‚             â”‚  â€¢ YouTube Data API v3       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Node.js 18+
- Python 3.11+
- PostgreSQL
- API keys: [OpenRouter](https://openrouter.ai), [YouTube Data API v3](https://console.cloud.google.com), [Polygon.io](https://polygon.io) (optional, for live stock data)

### Frontend

```bash
cd ai-information-hub
cp .env.example .env.local    # Add your API keys
npm install
npm run dev                   # http://localhost:3000
```

### Backend

```bash
cd ai-hub-backend
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env          # Add your API keys

python -m scripts.init_db --migrate-all
uvicorn app.main:app --reload # http://localhost:8000/docs
```

### Run Data Collection

```bash
# Daily collection (today)
python -m scripts.daily_collect

# Weekly collection (current week)
python -m scripts.weekly_collect

# Specific date/week
python -m scripts.daily_collect --date 2026-02-07
python -m scripts.weekly_collect --week 2026-kw06
```

## Tech Stack

| Layer | Technology |
|-------|-----------|
| **Frontend** | Next.js 16, React 19, Tailwind CSS 4, Shadcn/ui, TypeScript |
| **Backend** | FastAPI, SQLAlchemy, Alembic, PostgreSQL |
| **LLM Classification** | GLM-4.5-Air (OpenRouter, free tier) |
| **LLM Processing** | DeepSeek V3.2 (OpenRouter) |
| **Chat & Reports** | Aurora Alpha (OpenRouter) |
| **Stock Data** | Polygon.io API |
| **Hosting** | Vercel (frontend), Railway (backend + DB + cron) |
| **Design** | Instrument Serif, section-specific color accents, staggered animations |

## Data Pipeline

The backend processes news through a 4-stage pipeline:

| Stage | What happens | Output |
|-------|-------------|--------|
| **1. Fetch** | Collect from RSS, Hacker News, YouTube; filter by period boundaries | ~210 raw items |
| **2. Classify** | LLM classifies into tech/investment/tips (tips sources skip this) | Categorized pool |
| **3. Process** | Parallel LLM processing: generate bilingual summaries, extract entities | 30 tech + 21 investment + 15 tips + 5 videos |
| **4. Save** | Store in PostgreSQL, intersperse videos into tech feed | Database records |

Daily collections produce reduced counts (10 tech, 5 investment, 5 tips, 2 videos).

## API Reference

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/weeks` | GET | List periods (weeks with nested days) |
| `/api/tech/{periodId}` | GET | Tech feed with embedded videos |
| `/api/investment/{periodId}` | GET | Primary/Secondary/M&A data |
| `/api/tips/{periodId}` | GET | Curated tips |
| `/api/videos/{periodId}` | GET | YouTube video summaries |
| `/api/stock/{ticker}` | GET | Real-time stock data |
| `/api/stock/batch/?tickers=AAPL,NVDA` | GET | Batch stock data |
| `/api/admin/collect` | POST | Trigger full data collection |

Period IDs: daily `YYYY-MM-DD` or weekly `YYYY-kwWW`

Full API docs available at `/docs` (Swagger UI) when running the backend.

## Environment Variables

### Frontend (`ai-information-hub/.env.local`)

```bash
OPENROUTER_API_KEY=sk-or-v1-...     # For chat & report features
YOUTUBE_API_KEY=AIza...              # For video metadata
NEXT_PUBLIC_API_URL=http://localhost:8000/api  # Backend URL
```

### Backend (`ai-hub-backend/.env`)

```bash
DATABASE_URL=postgresql://user:pass@localhost:5432/aihub
OPENROUTER_API_KEY=sk-or-v1-...     # For LLM classification & processing
YOUTUBE_API_KEY=AIza...              # For video fetching
POLYGON_API_KEY=...                  # Optional: real-time stock data
ADMIN_API_KEY=your-secret-key       # Protects admin endpoints
CORS_ORIGINS=["http://localhost:3000"]
```

## Deployment

### Frontend â†’ Vercel

```bash
cd ai-information-hub
vercel --prod
```

Set environment variables in the Vercel dashboard. Auto-deploys on push to `main`.

### Backend â†’ Railway

```bash
cd ai-hub-backend
railway up
```

Railway auto-applies Alembic migrations on startup. Configure a cron job for daily collection at 22:00 UTC.

## Project Structure

```
DataCube-AI-Space/
â”œâ”€â”€ ai-information-hub/          # Frontend (Next.js)
â”‚   â”œâ”€â”€ app/                     # Pages + API routes
â”‚   â”‚   â”œâ”€â”€ api/chat/            # AI chat endpoint
â”‚   â”‚   â”œâ”€â”€ api/report/          # AI report generator
â”‚   â”‚   â”œâ”€â”€ [lang]/week/         # SSR week pages (SEO)
â”‚   â”‚   â””â”€â”€ feed.xml/            # Atom 1.0 feed
â”‚   â”œâ”€â”€ components/              # React components
â”‚   â”‚   â”œâ”€â”€ feeds/               # Tech, Investment, Tips feeds
â”‚   â”‚   â””â”€â”€ video-embed.tsx      # YouTube player
â”‚   â”œâ”€â”€ lib/                     # Utils, types, API client
â”‚   â””â”€â”€ middleware.ts            # Crawler bypass + welcome gate
â”‚
â”œâ”€â”€ ai-hub-backend/              # Backend (FastAPI)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ models/              # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ routers/             # API endpoints
â”‚   â”‚   â””â”€â”€ services/            # Business logic
â”‚   â”‚       â”œâ”€â”€ collector.py     # 4-stage pipeline
â”‚   â”‚       â”œâ”€â”€ llm_processor.py # Two-model LLM approach
â”‚   â”‚       â””â”€â”€ youtube_fetcher.py
â”‚   â”œâ”€â”€ alembic/                 # DB migrations
â”‚   â”œâ”€â”€ scripts/                 # CLI tools (daily/weekly collect)
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docs/                        # Translated READMEs
â””â”€â”€ LICENSE
```

## Contributing

Contributions are welcome! Here's how to get started:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

Please make sure your code passes the CI checks:
- **Frontend**: `tsc --noEmit` + `next build`
- **Backend**: `ruff check`

## License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**[Live Demo](https://www.datacubeai.space)** Â· **[Report Bug](https://github.com/Rswcf/DataCube-AI-Space/issues)** Â· **[Request Feature](https://github.com/Rswcf/DataCube-AI-Space/issues)**

If you find this project useful, please consider giving it a star!

</div>
