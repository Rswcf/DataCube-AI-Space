# DataCube AI Space

This repository contains multiple projects:

- **[ai-information-hub/CLAUDE.md](./ai-information-hub/CLAUDE.md)** - Frontend (Next.js + Vercel)
- **[ai-hub-backend/README.md](./ai-hub-backend/README.md)** - Backend API (FastAPI + Railway)

---

## AI Information Hub (Quick Reference)

Bilingual (DE/EN) weekly AI news aggregator with **YouTube video integration**. Curates tech breakthroughs, investment news, practical tips, and videos from RSS feeds + Hacker News + YouTube.

**Stack**: Next.js 16 + React 19 + Tailwind CSS 4 + Shadcn/ui (Frontend) | FastAPI + PostgreSQL (Backend)

**Status**: Full-stack implementation complete with Railway backend. **No authentication required**.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Backend (Railway) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                          â”‚
â”‚  ai-hub-backend/ (FastAPI + PostgreSQL)                  â”‚
â”‚       â†“                                                  â”‚
â”‚  Data Sources:                                           â”‚
â”‚    â€¢ RSS Feeds (22 sources)                              â”‚
â”‚    â€¢ Hacker News (Algolia API)                           â”‚
â”‚    â€¢ YouTube (Data API v3)                               â”‚
â”‚       â†“                                                  â”‚
â”‚  LLM Processing (OpenRouter):                            â”‚
â”‚    â€¢ Classifier: glm-4.5-air:free (classification)       â”‚
â”‚    â€¢ Processor: deepseek-v3.2 (content generation)       â”‚
â”‚       â†“                                                  â”‚
â”‚  PostgreSQL Database                                     â”‚
â”‚       â†“                                                  â”‚
â”‚  REST API: /api/tech, /api/investment, /api/tips...      â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Frontend (Vercel) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                          â”‚
â”‚  ai-information-hub/ (Next.js)                           â”‚
â”‚       â†“                                                  â”‚
â”‚  SPA (main page):                                        â”‚
â”‚    â€¢ Feed components (tech, investment, tips)             â”‚
â”‚    â€¢ Chat widget (week context for LLM)                  â”‚
â”‚    â€¢ Chat Assistant: glm-4.5-air:free                    â”‚
â”‚       â†“                                                  â”‚
â”‚  SSR pages (SEO/GEO):                                    â”‚
â”‚    â€¢ /week/[weekId] â€” full HTML + JSON-LD structured dataâ”‚
â”‚    â€¢ /feed.xml â€” Atom 1.0 feed (DE/EN)                   â”‚
â”‚    â€¢ /api/content-summary â€” Markdown for AI consumption  â”‚
â”‚    â€¢ /llms.txt â€” AI crawler site description              â”‚
â”‚                                                          â”‚
â”‚  Middleware: crawlers bypass login gate via UA detection  â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Directory Structure

```
DataCube_AI_Space/
â”œâ”€â”€ ai-information-hub/       # Frontend (Next.js)
â”‚   â”œâ”€â”€ app/                  # Pages + API routes
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/         # Chat assistant (glm-4.5-air:free)
â”‚   â”‚   â”‚   â””â”€â”€ content-summary/  # Markdown summary API (GEO)
â”‚   â”‚   â”œâ”€â”€ feed.xml/         # Atom 1.0 feed route
â”‚   â”‚   â””â”€â”€ week/[weekId]/    # SSR week pages (SEO)
â”‚   â”œâ”€â”€ components/           # React components
â”‚   â”‚   â”œâ”€â”€ feeds/            # Feed components
â”‚   â”‚   â””â”€â”€ video-embed.tsx   # YouTube player (next/image)
â”‚   â”œâ”€â”€ lib/                  # Utils, types, API client
â”‚   â”œâ”€â”€ middleware.ts         # Login gate + crawler bypass
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ data/             # Static JSON fallback
â”‚   â”‚   â”œâ”€â”€ llms.txt          # AI crawler site description
â”‚   â”‚   â””â”€â”€ robots.txt        # Crawler rules
â”‚   â””â”€â”€ .env.local            # API keys
â”‚
â””â”€â”€ ai-hub-backend/           # Backend (FastAPI)
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ models/           # SQLAlchemy models
    â”‚   â”œâ”€â”€ routers/          # API endpoints
    â”‚   â””â”€â”€ services/         # Business logic
    â”‚       â”œâ”€â”€ collector.py  # 4-stage pipeline
    â”‚       â”œâ”€â”€ youtube_fetcher.py
    â”‚       â””â”€â”€ llm_processor.py
    â”œâ”€â”€ alembic/              # DB migrations
    â”œâ”€â”€ scripts/              # CLI tools
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ railway.toml
```

---

## Data Collection Pipeline

The backend uses a 4-stage pipeline:

| Stage | Description |
|-------|-------------|
| **Stage 1** | Fetch raw data (RSS, HN, YouTube) + ISO week boundary filter |
| **Stage 2** | Classify articles (tips sources skip classification) |
| **Stage 3** | Parallel LLM processing (tech, investment, tips, videos) |
| **Stage 4** | Save to PostgreSQL |

**Tips sources** (14 Reddit communities + 2 blogs) bypass LLM classification and are processed directly as tips.

> ðŸ“– **For detailed data pipeline documentation with complete flow diagrams**, see [ai-hub-backend/README.md](./ai-hub-backend/README.md#complete-data-pipeline-deep-dive)

---

## Quick Commands

### Frontend
```bash
cd ai-information-hub
npm run dev                   # Dev server :3000
vercel --prod                 # Deploy to production
```

### Backend (Local)
```bash
cd ai-hub-backend
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
python -m scripts.init_db --migrate-all
uvicorn app.main:app --reload   # API docs: :8000/docs
```

### Data Collection
```bash
# Via API (set ADMIN_API_KEY env var or use Railway dashboard value)
curl -X POST "https://api-production-3ee5.up.railway.app/api/admin/collect?week_id=2026-kw05" \
  -H "X-API-Key: $ADMIN_API_KEY"

# Process only (reuse raw data)
curl -X POST "https://api-production-3ee5.up.railway.app/api/admin/collect/process?week_id=2026-kw05" \
  -H "X-API-Key: $ADMIN_API_KEY"

# M&A only (reprocess M&A without affecting other sections)
curl -X POST "https://api-production-3ee5.up.railway.app/api/admin/collect/ma?week_id=2026-kw05" \
  -H "X-API-Key: $ADMIN_API_KEY"
```

---

## Environment Variables

### Frontend (.env.local)
```bash
OPENROUTER_API_KEY=sk-or-v1-...
YOUTUBE_API_KEY=AIza...
NEXT_PUBLIC_API_URL=https://api-production-3ee5.up.railway.app/api
```

### Backend (.env or Railway)
```bash
DATABASE_URL=postgresql://...
OPENROUTER_API_KEY=sk-or-v1-...
YOUTUBE_API_KEY=AIza...
POLYGON_API_KEY=...              # Polygon.io API for real-time stock data
ADMIN_API_KEY=<set-in-railway-dashboard>
CORS_ORIGINS=["http://localhost:3000","https://www.datacubeai.space","https://ai-information-hub.vercel.app"]
```

---

## API Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /api/weeks` | List weeks |
| `GET /api/tech/{weekId}` | Tech + videos |
| `GET /api/investment/{weekId}` | Investment |
| `GET /api/tips/{weekId}` | Tips (15 DE + 15 EN) |
| `GET /api/videos/{weekId}` | Videos only |
| `GET /api/stock/{ticker}` | Real-time stock data (Polygon.io) |
| `GET /api/stock/batch/?tickers=AAPL,NVDA` | Batch stock data |
| `GET /api/stock/formatted/{ticker}?language=en` | Pre-formatted stock data |
| `POST /api/admin/collect` | Full collection |
| `POST /api/admin/collect/process` | Reprocess raw data |
| `POST /api/admin/collect/ma` | M&A-only reprocessing |

---

## Deployment

**Frontend**: Vercel
- Production: https://www.datacubeai.space
- Deploy: `vercel --prod`

**Backend**: Railway
- Production: https://api-production-3ee5.up.railway.app
- Deploy: `railway up`

---

## LLM Models

| Purpose | Model | Provider |
|---------|-------|----------|
| Classification | `z-ai/glm-4.5-air:free` | OpenRouter (free) |
| Content Processing | `deepseek/deepseek-v3.2` | OpenRouter |
| Chat Assistant | `z-ai/glm-4.5-air:free` | OpenRouter (free) |

---

## Claude-Codex Collaboration Protocol

### Overview
`.ai-collab/` is an interaction platform for asynchronous collaboration between Claude and Codex CLI.

### Directory Structure
```
.ai-collab/
â”œâ”€â”€ requests/     # Claude â†’ Codex task requests
â”œâ”€â”€ responses/    # Codex â†’ Claude task responses
â”œâ”€â”€ context/      # Shared project context
â””â”€â”€ archive/      # Completed task archives
```

### Usage

**1. Create Request File**:
Filename format: `YYYYMMDD-HHMMSS-{type}-{brief}.md`

Types: `review` | `fix` | `explore` | `generate` | `refactor` | `test`

```markdown
# Request: Review collector.py error handling

**Type**: review
**Priority**: High
**Status**: PENDING

## Task Description
Review error handling in collector.py stage1_fetch_and_store function.

## Target Files
- `ai-hub-backend/app/services/collector.py`

## Expected Output
List of issues with line numbers and severity.
```

**2. Execute with Codex**:
```bash
codex exec --skip-git-repo-check -s read-only \
  -o .ai-collab/responses/20260203-143000-review-collector-response.md \
  "Read .ai-collab/requests/20260203-143000-review-collector.md and execute the task."
```

**3. Read Response**:
Response file will be created at `.ai-collab/responses/`

### Response Statuses
- `COMPLETED` - Task finished successfully
- `NEEDS_INPUT` - Codex needs more information from Claude
- `FAILED` - Task could not be completed

### Best Practices
- Check `context/` files for shared project knowledge
- Keep requests focused on single tasks
- Archive completed request/response pairs to `archive/`

See `.ai-collab/README.md` for full protocol documentation.
